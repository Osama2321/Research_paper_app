{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNFDk3toWBdseW3+PJLZ1TI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Osama2321/Research_paper_app/blob/main/Research_Paper_app.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "import pdfplumber\n",
        "import faiss\n",
        "import numpy as np\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from groq import Groq\n",
        "import os\n",
        "\n",
        "# Set API Key for Groq (Make sure you set it in your environment)\n",
        "os.environ['GROQ_API_KEY'] = \"your_groq_key_here\"\n",
        "\n",
        "# Load Embedding Model\n",
        "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# Function to Extract Text from PDF\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    text = \"\"\n",
        "    with pdfplumber.open(pdf_path) as pdf:\n",
        "        for page in pdf.pages:\n",
        "            page_text = page.extract_text()\n",
        "            if page_text:\n",
        "                text += page_text + \"\\n\"\n",
        "    return text.strip()\n",
        "\n",
        "# Function to Chunk Text\n",
        "def chunk_text(text, chunk_size=500, overlap=50):\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=overlap)\n",
        "    return text_splitter.split_text(text)\n",
        "\n",
        "# Function to Store Chunks in FAISS\n",
        "def store_in_faiss(chunks):\n",
        "    embeddings = embedding_model.encode(chunks)\n",
        "    dimension = embeddings.shape[1]\n",
        "    index = faiss.IndexFlatL2(dimension)\n",
        "    index.add(np.array(embeddings))\n",
        "    return index, chunks\n",
        "\n",
        "# Function to Retrieve Relevant Chunks\n",
        "def query_faiss(query, index, chunks, top_k=3):\n",
        "    query_embedding = embedding_model.encode([query])\n",
        "    distances, indices = index.search(query_embedding, top_k)\n",
        "    retrieved_texts = [chunks[i] for i in indices[0]]\n",
        "    return \" \".join(retrieved_texts)\n",
        "\n",
        "# Function to Get Response from Groq API\n",
        "def get_groq_summary(query, context):\n",
        "    api_key = os.getenv(\"GROQ_API_KEY\")\n",
        "    if not api_key:\n",
        "        return \"Error: Groq API Key not found. Set it in your environment variables.\"\n",
        "\n",
        "    client = Groq(api_key=api_key)\n",
        "\n",
        "    chat_completion = client.chat.completions.create(\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are an AI that summarizes research papers and extracts key points.\"},\n",
        "            {\"role\": \"user\", \"content\": f\"Context: {context}\\n\\n{query}\"}\n",
        "        ],\n",
        "        model=\"llama-3.3-70b-versatile\"\n",
        "    )\n",
        "    return chat_completion.choices[0].message.content\n",
        "\n",
        "# Streamlit UI\n",
        "st.set_page_config(page_title=\"RAG Research Paper App\", layout=\"wide\")\n",
        "st.title(\"ðŸ“„ RAG Research Paper Summarizer\")\n",
        "\n",
        "uploaded_file = st.file_uploader(\"Upload Research Paper (PDF)\", type=\"pdf\")\n",
        "\n",
        "if uploaded_file:\n",
        "    with open(\"temp.pdf\", \"wb\") as f:\n",
        "        f.write(uploaded_file.getbuffer())\n",
        "\n",
        "    st.success(\"âœ… File uploaded successfully!\")\n",
        "\n",
        "    # Extract and Process Text\n",
        "    text = extract_text_from_pdf(\"temp.pdf\")\n",
        "    chunks = chunk_text(text)\n",
        "    index, stored_chunks = store_in_faiss(chunks)\n",
        "\n",
        "    st.success(\"âœ… Text extracted and indexed successfully!\")\n",
        "\n",
        "    # User Query\n",
        "    query = st.text_input(\"ðŸ”Ž Ask something about this paper:\")\n",
        "\n",
        "    if st.button(\"Generate Summary\"):\n",
        "        context = query_faiss(query, index, stored_chunks)\n",
        "        summary = get_groq_summary(query, context)\n",
        "\n",
        "        st.subheader(\"ðŸ“Œ Summary:\")\n",
        "        st.write(summary)\n",
        "\n",
        "        # Create download button\n",
        "        summary_filename = \"summary.txt\"\n",
        "        with open(summary_filename, \"w\") as f:\n",
        "            f.write(summary)\n",
        "\n",
        "        with open(summary_filename, \"rb\") as f:\n",
        "            st.download_button(\n",
        "                label=\"ðŸ“¥ Download Summary\",\n",
        "                data=f,\n",
        "                file_name=\"research_summary.txt\",\n",
        "                mime=\"text/plain\"\n",
        "            )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YBZ5x-1q0pwN",
        "outputId": "86a86ac9-cfdd-48be-ca04-655d88f776f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-03-23 16:50:39.345 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-03-23 16:50:39.346 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-03-23 16:50:39.348 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-03-23 16:50:39.349 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-03-23 16:50:39.350 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-03-23 16:50:39.350 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-03-23 16:50:39.351 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-03-23 16:50:39.352 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q -O - ipv4.icanhazip.com"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WHH2ZWEm0pzW",
        "outputId": "a7ed421a-b918-4029-a1b3-d76f996ddf4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34.125.171.119\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py & npx localtunnel --port 8501"
      ],
      "metadata": {
        "id": "cTLnTMs90p_L"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1hzlibOq0qAU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}